# ğŸ” AQA Search - Ask a Question from Your Files (Offline RAG App)

**AQA Search** is a **local Retrieval-Augmented Generation (RAG) application** built using **Streamlit**, **LangChain**, **Ollama**, and **ChromaDB**. This app allows you to upload files, paste URLs (optional), or enter custom text to ask questions and get context-aware answers â€” all running locally on your machine **without internet**.

![AQA Search Screenshot](AQA.png)

---

## ğŸ§  Features

- ğŸ“‚ Upload multiple files (PDF, DOCX, TXT, HTML)
- ğŸŒ Enter URLs (âš ï¸ available only when hosted online)
- âœï¸ Paste your own text for Q&A
- ğŸ§  Uses local **LLM (via Ollama)** and **embeddings** (`nomic-embed-text`)
- ğŸ” Retrieves best context using **ChromaDB**, reranked with **CrossEncoder**
- ğŸ“¡ No internet or API keys required â€” 100% offline

---

## ğŸ› ï¸ Built With

- [Streamlit](https://streamlit.io/)
- [LangChain](https://www.langchain.com/)
- [ChromaDB](https://www.trychroma.com/)
- [Ollama](https://ollama.com/)
- [nomic-embed-text](https://ollama.com/library/nomic-embed-text)
- [LLaMA3 (or other local LLMs)](https://ollama.com/library)

---

## ğŸš€ How to Run Locally

> âœ… You must have **Ollama** installed and running with required models locally before launching the app.

### ğŸ“¥ Step 1: Clone the repository

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
ğŸ“¦ Step 2: Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
Or manually:

bash
Copy
Edit
pip install streamlit langchain chromadb sentence-transformers ollama requests unstructured pypdf python-docx html5lib
ğŸ¤– Step 3: Start Ollama with required models
bash
Copy
Edit
ollama serve
ollama run llama3
ollama run nomic-embed-text
Replace llama3 with the model name youâ€™re using (e.g., mistral, llama2, etc.)

ğŸšª Step 4: Run the Streamlit app
bash
Copy
Edit
streamlit run aqa_app.py
Then open your browser and go to: http://localhost:8501

ğŸ“Œ Notes
âœ… This app is offline-first and runs fully on your local system using Ollama and ChromaDB.

ğŸŒ The URL-based input works only when deployed online (e.g., Streamlit Cloud, Hugging Face).

ğŸ§  Document embeddings are stored locally in a chroma_db/ folder.

ğŸ”® Future Improvements
ğŸŒ Enable online URL scraping (when deployed)

ğŸ’¾ Export Q&A results as PDF or CSV
ğŸ§  Add memory to track user context
ğŸ“š Combine results from multiple files

